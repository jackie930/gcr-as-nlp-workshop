[{"uri":"/01_hf_model_zoo.html","title":"动手实验1: 利用huggingface直接进行模型部署","tags":[],"description":"","content":"quick start cd SageMaker git clone https://github.com/jackie930/notebooks.git 接下来我们进行模型的训练实验\n Experiment 1: Fill Mask： /home/ec2-user/SageMaker/notebooks/sagemaker/15_use_hf_model_directly/1_fill_mask.ipynb运行 Experiment 2: Translation /home/ec2-user/SageMaker/notebooks/sagemaker/15_use_hf_model_directly/2_translation.ipynb运行 Experiment 3: Image Classification /home/ec2-user/SageMaker/notebooks/sagemaker/15_use_hf_model_directly/3_image_classification.ipynb运行 Experiment 4: Batch Transform /home/ec2-user/SageMaker/notebooks/sagemaker/15_use_hf_model_directly/4_batch_transform.ipynb运行 Experiment 5: Named Entity Recognition (NER) /home/ec2-user/SageMaker/notebooks/sagemaker/15_use_hf_model_directly/5_token_classification.ipynb运行  "},{"uri":"/02classification.html","title":"动手实验2: 利用huggingface进行文本分类模型训练","tags":[],"description":"","content":"文本分类任务作为最基础的任务，我们既可以用机器学习的方式，也可以用深度学习的方式进行训练，部署。\n下面的结果为我们模型对比实验的结果：\n   Model Train sample Num_class Train loss instance train-time epoch eval loss eval f1     Bert-base 180000 31 0.09 ml.p3.16xlarge 8h 5 1.26 0.67   Distillbert-base 180000 31 0.1489 ml.p3.16xlarge 3h 5 1.26 0.67   Xlm-Roberta-base 180000 31 0.9821 ml.p3.16xlarge 40min 1 0.78 0.79   Lightgbm (benchmark) 180000 31 0.56 ml.m5.xlarge 40min 5 1.19 0.71    quick start 下载代码 (如果已经进行过实验1，则无需重复下载代码)\ncd SageMaker git clone https://github.com/jackie930/notebooks.git 接下来我们进行模型的训练实验\n 机器学习模型： /home/ec2-user/SageMaker/notebooks/sagemaker/14_train_text_calssfication/lightgbm.ipynb运行 基于transformer的深度学习模型 /home/ec2-user/SageMaker/notebooks/sagemaker/14_train_text_calssfication/train-huggingface.ipynb运行  我们可以看到，通过huggingface api，我们可以通过更换不同的模型pretrain训练得到不同的结果。同样数据集，在roberta架构下表现最好\n"},{"uri":"/03mt5.html","title":"动手实验3: 基于Amazon SageMaker的MT5中文摘要模型训练动手实验","tags":[],"description":"","content":"模型架构 Transformer For Text Summary：T5 (JMLR): 旨在开发NLP通用模型：将多个NLP下游任务抽象为text-to-text的任务，不同的任务使用不同的profix来代表，由于任务类型相同，故可以协同训练。\nmT5: A massively multilingualpre-trained text-to-text transformer 2021 google research\n大规模多语言T5预训练语言模型mT5，在覆盖101种语言的新的Common Crawl数据集上进行预训练，可直接适用于多语言场景，在各种基准测试集上展现出强大的性能。\nreference  paper： https://arxiv.org/pdf/2010.11934.pdf source code：https://github.com/google-research/multilingual-t5  @inproceedings{xue-etal-2021-mt5, title = \u0026ldquo;m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer\u0026rdquo;, author = \u0026ldquo;Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin\u0026rdquo;, booktitle = \u0026ldquo;Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\u0026rdquo;, month = jun, year = \u0026ldquo;2021\u0026rdquo;, address = \u0026ldquo;Online\u0026rdquo;, publisher = \u0026ldquo;Association for Computational Linguistics\u0026rdquo;, url = \u0026ldquo;https://aclanthology.org/2021.naacl-main.41\u0026quot;, doi = \u0026ldquo;10.18653/v1/2021.naacl-main.41\u0026rdquo;, pages = \u0026ldquo;483\u0026ndash;498\u0026rdquo; }\n"},{"uri":"/04_pegasus.html","title":"动手实验4: 基于Amazon SageMaker的Pegasus摘要模型训练动手实验","tags":[],"description":"","content":"模型架构 Pegasus是一个标准的Transformer架构，其全称是：利用提取的间隙句进行摘要概括的预训练模型（Pre-training with Extracted Gap-sentences for Abstractive Summarization）。就是设计一种间隙句生成的自监督预训练目标（GSG），来改进生成摘要的微调性能。\nGSG预训目标  Mask掉整个句子 将Mask的句子拼接成摘要 使用上下文补全  Mask策略\n模型表现 Pegasus-large在各个摘要数据集上的结果同之前的SOTA对比：在所有数据集上均得到了SOTA的性能表现。\nreference  paper： https://arxiv.org/abs/1912.08777 source code：https://github.com/google-research/pegasus  @misc{zhang2019pegasus, title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu}, year={2020}, eprint={1912.08777}, archivePrefix={ICML}, primaryClass={cs.CL} }\n"},{"uri":"/04_pegasus/0501train.html","title":" Pegasus模型训练","tags":[],"description":"","content":"我们现在会用sagemaker进行一个pegasus模型的本地训练，使用ML.P3.2xlarge机型。\n数据准备 首先下载代码\ncd SageMaker git clone https://github.com/HaoranLv/nlp_transformer.git 然后打开文件 hp_main.ipynb，逐行运行。\n首先安装环境\n然后处理数据hp_data.ipynb，切分train/test。 注意这里，为了快速产生结果，我们只要用1000条数据训练，100条测试/验证\ntrain_df.to_csv('./data/hp/summary/news_summary_cleaned_train.csv',index=False) test_df.to_csv('./data/hp/summary/news_summary_cleaned_test.csv',index=False) train_df[:1000].to_csv('./data/hp/summary/news_summary_cleaned_small_train.csv',index=False) test_df[:100].to_csv('./data/hp/summary/news_summary_cleaned_small_test.csv',index=False) 模型训练 接下来我们运行训练，首先下载我们已经训练好的模型\n!mkdir -p models/pretrain/pegasus !mkdir -p models/pretrain/bart !mkdir -p ./models/local_train/pegasus-hp !mkdir -p ./models/local_train/bart-hp !aws s3 cp s3://datalab2021/hupo_nlp/models/pegasus/checkpoint-46314.zip models/pretrain/pegasus !aws s3 cp s3://datalab2021/hupo_nlp/models/bart/checkpoint-46314.zip models/pretrain/bart !unzip models/pretrain/pegasus/checkpoint-46314.zip -d models/pretrain/pegasus \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 !unzip models/pretrain/bart/checkpoint-46314.zip -d models/pretrain/bart \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 然后进行模型训练，这里没有用我们训练好的模型，而是使用huggingface上的公开的pegasus-large作为训练起点，为了演示目的，我们只运行一个epoch，大约需要5min\n!python -u examples/pytorch/summarization/run_summarization.py \\ --model_name_or_path google/pegasus-large \\ --do_train \\ --do_eval \\ --per_device_train_batch_size=2 \\ --per_device_eval_batch_size=1 \\ --save_strategy epoch \\ --evaluation_strategy epoch \\ --overwrite_output_dir \\ --predict_with_generate \\ --train_file './data/hp/summary/news_summary_cleaned_small_train.csv' \\ --validation_file './data/hp/summary/news_summary_cleaned_small_test.csv' \\ --text_column 'text' \\ --summary_column 'headlines' \\ --output_dir='./models/local_train/pegasus-hp' \\ --num_train_epochs=1.0 \\ --eval_steps=500 \\ --save_total_limit=3 \\ --source_prefix \u0026quot;summarize: \u0026quot; \u0026gt; train_pegasus.log 训练完成后，会提示日志信息如下\n train  eval   模型结果文件及相应的日志等信息会自动保存在./models/local_train/pegasus-hp/checkpoint-500\n结果本地测试 我们可以直接用这个产生的模型文件进行本地推理。注意这里的模型文件地址的指定为你刚刚训练产生的。\nimport pandas as pd df=pd.read_csv('./data/hp/summary/news_summary_cleaned_small_test.csv') print('原文:',df.loc[0,'text']) print('真实标签:',df.loc[0,'headlines']) from transformers import pipeline summarizer = pipeline(\u0026quot;summarization\u0026quot;, model=\u0026quot;./models/local_train/pegasus-hp/checkpoint-500\u0026quot;) print('模型预测:',summarizer(df.loc[0,'text'], max_length=50)[0]['summary_text']) 输出如下\n原文: Germany on Wednesday accused Vietnam of kidnapping a former Vietnamese oil executive Trinh Xuan Thanh, who allegedly sought asylum in Berlin, and taking him home to face accusations of corruption. Germany expelled a Vietnamese intelligence officer over the suspected kidnapping and demanded that Vietnam allow Thanh to return to Germany. However, Vietnam said Thanh had returned home by himself. 真实标签: Germany accuses Vietnam of kidnapping asylum seeker 模型预测: Germany accuses Vietnam of kidnapping ex-oil exec, taking him home 到这里，就完成了一个模型的训练过程。\n"},{"uri":"/03mt5/0301train.html","title":"MT5模型实验","tags":[],"description":"","content":"首先下载代码, ！注意，如果你已经执行过实验2bart，无需重复下载，直接打开即可。\ncd SageMaker git clone https://github.com/jackie930/notebooks.git 然后打开/home/ec2-user/SageMaker/notebooks/sagemaker/08_distributed_summarization_bart_t5/train-huggingface.ipynb运行。 注意在运行前，需要将待训练的数据‘meta_description.parquet' 上传到同样的文件夹内。\n训练开启后，可以看到产生了对应的训练日志\n及对应的训练任务\n大约25min完成训练，结束后可以直接部署模型文件为endpoint并进行推理。注意这里只用了1000条数据训练1轮，所以模型效果不佳。\n"},{"uri":"/04_pegasus/0502train.html","title":"Pegasus模型增强训练","tags":[],"description":"","content":"在完成本地训练后，接下来我们模拟一个增强训练过程。在我们刚才完成的训练任务中，产生了模型文件./models/local_train/pegasus-hp/checkpoint-500，然后，我们运行\n!python -u examples/pytorch/summarization/run_summarization.py \\ --model_name_or_path models/pretrain/pegasus/checkpoint-46314 \\ --do_train \\ --do_eval \\ --per_device_train_batch_size=2 \\ --per_device_eval_batch_size=1 \\ --save_strategy epoch \\ --evaluation_strategy epoch \\ --overwrite_output_dir \\ --predict_with_generate \\ --train_file './data/hp/summary/news_summary_cleaned_small_train.csv' \\ --validation_file './data/hp/summary/news_summary_cleaned_small_test.csv' \\ --text_column 'text' \\ --summary_column 'headlines' \\ --output_dir='./models/local_train/pegasus-hp' \\ --num_train_epochs=1.0 \\ --eval_steps=500 \\ --save_total_limit=3 \\ --source_prefix \u0026quot;summarize: \u0026quot; \u0026gt; train_pegasus_2.log 我们可以看到，在训练时，日志中有如下的记录loading weights file models/pretrain/pegasus/checkpoint-46314/pytorch_model.bin说明模型是导入了一个之前训练的基础版本，也可以通过训练的loss值以及最终的rouge指标判断出这个结果是增强训练产生的。\n同样的，我们可以利用本地推理进行测试。\n本地测试，效果提升明显\nimport pandas as pd df=pd.read_csv('./data/hp/summary/news_summary_cleaned_small_test.csv') print('原文:',df.loc[0,'text']) print('真实标签:',df.loc[0,'headlines']) from transformers import pipeline summarizer = pipeline(\u0026quot;summarization\u0026quot;, model=\u0026quot;./models/local_train/pegasus-hp/checkpoint-500\u0026quot;) print('模型预测:',summarizer(df.loc[0,'text'], max_length=50)[0]['summary_text']) 输出\n原文: Germany on Wednesday accused Vietnam of kidnapping a former Vietnamese oil executive Trinh Xuan Thanh, who allegedly sought asylum in Berlin, and taking him home to face accusations of corruption. Germany expelled a Vietnamese intelligence officer over the suspected kidnapping and demanded that Vietnam allow Thanh to return to Germany. However, Vietnam said Thanh had returned home by himself. 真实标签: Germany accuses Vietnam of kidnapping asylum seeker 模型预测: Germany accuses Vietnam of kidnapping ex-oil exec, taking him home "},{"uri":"/04_pegasus/0503deploy.html","title":"Pegasus模型部署","tags":[],"description":"","content":"首先打包镜像并推送\n!sh build_and push.sh pegasus-hp 然后部署一个预置的endpoint\n#注意修改：847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/pegasus-hp为自己对应的 %cd endpoint !python create_endpoint.py \\ --endpoint_ecr_image_path \u0026#34;847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/pegasus-hp\u0026#34; \\ --endpoint_name \u0026#39;pegasus\u0026#39; \\ --instance_type \u0026#34;ml.p3.2xlarge\u0026#34; %cd .. 输出\nmodel_name: pegasus endpoint_ecr_image_path: 847380964353.dkr.ecr.ap-northeast-1.amazonaws.com/pegasus-hp \u0026lt;\u0026lt;\u0026lt; Completed model endpoint deployment. pegasus 当状态变为InService即代表部署完成\n在部署结束后，看到SageMaker控制台生成了对应的endpoint,可以使用如下客户端代码测试调用\n%%time from boto3.session import Session import json df=pd.read_csv(\u0026#39;./data/hp/summary/news_summary_cleaned_small_test.csv\u0026#39;) print(\u0026#39;原文:\u0026#39;,df.loc[0,\u0026#39;text\u0026#39;]) print(\u0026#39;真实标签:\u0026#39;,df.loc[0,\u0026#39;headlines\u0026#39;]) data={\u0026#34;data\u0026#34;: df.loc[0,\u0026#39;text\u0026#39;]} session = Session() runtime = session.client(\u0026#34;runtime.sagemaker\u0026#34;) response = runtime.invoke_endpoint( EndpointName=\u0026#39;pegasus\u0026#39;, ContentType=\u0026#34;application/json\u0026#34;, Body=json.dumps(data), ) result = json.loads(response[\u0026#34;Body\u0026#34;].read()) print (result) 结果如下\n{'result': '[SEP] [CLS] Germany accuses Vietnam of kidnapping asylum seeker [SEP]', 'infer_time': '0:00:02.948025'} CPU times: user 169 ms, sys: 30.9 ms, total: 200 ms Wall time: 3.42 s "},{"uri":"/","title":"10 lines code to be an NLP expert!","tags":[],"description":"","content":"Author\n JUNYI LIU (AWS GCR SR.Applied Scientist) HENAN WANG (AWS GCR SR.Applied Scientist) HAORAN LV (AWS GCR Applied Scientist)  概述 本次workshop分为几个部分\n 利用huggingface预训练模型部署使用  模型训练 模型部署   基于Amazon SageMaker的文本分类模型训练动手实验  模型训练 模型部署   基于Amazon SageMaker的MT5模型训练动手实验  启动模型训练任务 模型部署   基于Amazon SageMaker的Pegasus模型训练动手实验  模型训练 模型部署    本次 workshop 前提 本次 workshop 建议在 US-WEST-2 Region 使用。为了演示方便，所以本 workshop 所有的演示都会以US-WEST-2 Region 为例。\n"},{"uri":"/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags.html","title":"Tags","tags":[],"description":"","content":""}]